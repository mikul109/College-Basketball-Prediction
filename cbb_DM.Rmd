---
title: '**College Basketball Prediction**'
output:
  html_notebook: default
  pdf_document: default
---
### Data Mining Final Project

**By: Mikul Muzumdar, Alec Plante, Johnson Feng, Leon Masin, Sumukh Shankar**

Data from https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset

`GitHub repo: https://github.com/mikul109/College-Basketball-Prediction`

```{r}
# Dependencies:
library(tidyverse)
library(data.table)
library(classInt)
library(corrplot)
library(leaps)
library(nnet)
library(e1071)
library(partykit)
library(class)
library(xgboost)
library(caret)
```


<br>
 
| **Objective:**
|   Use historical college basketball data to predict Regular Season Wins

| **Steps:**
|     I. [Explore Dataset](#I)
|     II. [Data Preprocessing](#II)
|         1. [Cleaning](#II.1)
|         2. [Discretization](#II.2)
|         3. [Reduce Dimensionality](#II.3)
|     III. [Build and Test Classification Models](#III)
|         1. [Logistic Regression](#III.1)
|         2. [Support Vector Machine](#III.2)
|         3. [Decision Tree](#III.3)
|         4. [K-Nearest-Neighbors](#III.4)
|         5. [XGBoost](#III.5)
|     IV. [Visualize and Analyze results](#IV)

<br>


## I. Data Exploration {#I}

```{r}
# load data
cbb <- read.csv("https://raw.githubusercontent.com/mikul109/College-Basketball-Prediction/main/cbb.csv")
setDT(cbb)
```
Data from the 2013, 2014, 2015, 2016, 2017, 2018, and 2019 D1 College Basketball seasons

```{r}
dim(cbb) 
```
This data has 2455 rows and 24 columns


```{r}
str(cbb)
```
This shows the type of data and a preview of the data in each column for the dataset. Most of the variables are numeric, but there are also integers and character data types.


```{r}
head(cbb)
```
The head function displays the first 6 rows of the data set.

```{r}
summary(cbb)
```
The summary function of used on the data set shows the minimum and maximum number of each column, as well as the 1st quartile, 3rd quartile, the mean and the median. 
This function reveals some important information about our dataset.
- Not every teams plays the same number of games like in many sports.
- NOt every team in this data set is ranked (no Seed value). This means that they did not make it to the tournament




```{r}
summary(cbb$YEAR)
```
Because COVID messed everything up after 2019, we will use data from 2013 to 2019

```{r}
summary(cbb$W) 
```
The average number of wins is 16.28, Max = 38

```{r}
boxplot(W~YEAR, data = cbb) 
```
Distribution of wins based on the season. These seem to be pretty consistent from year to year



```{r}
boxplot(W~SEED, data = cbb) 
```
This Boxplot shows the distribution of wins based on the seed value. Interestingly, the median number of wins does not always decrease with the seed number. For instance, the median wins for a 12th seed is higher than that for the 5th seed.

This could be due to more games. I am going to create a variable called win percentage and analyze that to see if there are any differences.

```{r}
cbb1<- cbb
cbb1$WPERC<-cbb1$W/cbb1$G

boxplot(WPERC~SEED, data = cbb1) 
```
The same pattern continues. It seems to be even a little more exagerated with the 12th seed. Lets view the numbers used in this graph
```{r}
by(cbb1$WPERC,cbb1$SEED,summary)
```


```{r}
boxplot(G~YEAR, data = cbb) 
```
Same but for games. As you can see, there is a median of about 31 games per year


## II. Data preprocessing {#II}

### 1. Data Cleaning {#II.1}

```{r}
colSums(is.na(cbb))
```
The only N/A's are in POSTEASON and SEED. Since they only apply to a small percentage of teams that made the tournament, we should drop these columns from our model data


```{r}
# drop POSTSEASON and SEED
cbb2 <- subset(cbb1, select=-c(POSTSEASON, SEED))
```

### 2. Discretization of the Class Variable {#II.2}

```{r}
# Our class variable is win percentage in the regular season, WPERC
# We will use equal frequency binning with 4 bins
nbins <- 4
bins <- classIntervals(cbb2$WPERC, nbins, style = 'quantile')
bins
```
```{r}
# assign bins to the WPERC variable
# assign variables as 1-4, representing the 4 bins
cbb2$WPERC_b <- cut(cbb2$WPERC, breaks = bins$brks, labels=as.character(1:nbins))
summary(cbb2$WPERC_b)
```
```{r}
# drops rows that did not fit into bins
cbb2<-cbb2[!(cbb2$WPERC_b=="NA"),]
summary(cbb2$WPERC_b)

# drop the WPERC variable that is not binned
cbb2<-subset(cbb2, select=-c(WPERC))
```



### 3. Reducing Dimensionality/Variable Selection {#II.3}

| Our Class Variable:
| `WPERC_b: team's win percentage (number of games won divided by number games played), in 4 bins`

| Other Variables:
| `TEAM: The Division I college basketball school`  
| `CONF: The Athletic Conference in which the school participates in`
| `G: Number of games played`
| `W: Number of games won`
| `ADJOE: Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions)`
|               `a team would have against the average Division I defense)`
| `ADJDE: Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions)`
|               `a team would have against the average Division I offense)`
| `BARTHAG: Power Rating (Chance of beating an average Division I team)`
| `EFG_O: Effective Field Goal Percentage Shot`
| `EFG_D: Effective Field Goal Percentage Allowed`
| `TOR: Turnover Percentage Allowed (Turnover Rate)`
| `TORD: Turnover Percentage Committed (Steal Rate)`
| `ORB: Offensive Rebound Rate`
| `DRB: Offensive Rebound Rate Allowed`
| `FTR : Free Throw Rate (How often the given team shoots Free Throws)`
| `FTRD: Free Throw Rate Allowed`
| `2P_O: Two-Point Shooting Percentage`
| `2P_D: Two-Point Shooting Percentage Allowed`
| `3P_O: Three-Point Shooting Percentage`
| `3P_D: Three-Point Shooting Percentage Allowed`
| `ADJ_T: Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team` 
|               `that wants to play at an average Division I tempo)`
| `WAB: Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)`
| `YEAR: Season`


<br>
First, we should determine which variables outside of WPERC are independent. 
```{r fig.width=10}
# remove class and string variables
cbb_var <- subset(cbb2, select = -c(TEAM, CONF, WPERC_b))

# remove obvious dependent variables
cbb_var1 <- subset(cbb_var, select = -c(W, G, WAB, BARTHAG))

# get correlations between each other 
cbb_cor <- cor(cbb_var1)

# plot
corrplot(cbb_cor, method="circle")
```

Next, we should see which variables are correlated with the class variable. 
```{r fig.width=10}
# run linear regression and plot r^2
m1<-regsubsets(WPERC_b~.-TEAM-YEAR-CONF-G-W-WAB-BARTHAG, data = cbb2)
plot(m1,scale="adjr2", main="Variable Selection Based on adjr2")
```

| Conclusions:
| - idk


<br>

| Final Variables, based on independence, correlation with WPERC, and general basketball knowledge:
| 1. EFG_O
| 2. EFG_D
| 3. TOR
| 4. TORD
| 5. ORB


## III. Build and Test Classification Models {#III}

Split into Train and Test datasets
```{r}
# convert class variable to factor
cbb2$WPERC_b <- as.factor(cbb2$WPERC_b)

# subset with needed variables
cbb_mod <- subset(cbb2, select=c(WPERC_b, EFG_O, EFG_D, TOR, TORD, ORB))

# randomly split data 70/30
set.seed(123)
ind <- sample(2, nrow(cbb_mod), replace=TRUE, prob=(c(0.7,0.3)))
train_data <- cbb_mod[ind==1,]
test_data <- cbb_mod[ind==2,]
```


Before running models, I am going to create a baseline accuracy. Since this dataset is dominated by teams not making the tournament, the results from the models should be more than this percent:
```{r}
###### not needed?
View(cbb)
nrow(cbb[cbb$SEED==100])/nrow(cbb)
```

### 1. Logistic Regression {#III.1}

Build Model
```{r}
# build model
cbb_glm <- multinom(WPERC_b ~ ., data = train_data)

# call model
summary(cbb_glm)

```

Test Model
```{r}
# Make predictions
pred_classes_glm <- cbb_glm %>% predict(test_data)
# Model accuracy
mean(pred_classes_glm == test_data$WPERC_b)
```


### 2. SVM {#III.2}

Build Model
```{r}
cbb_svm = svm(WPERC_b ~ ., data = train_data, type="C-classification", kernal="radial", gamma=0.1, cost=10, scale = TRUE)

summary(cbb_svm)
```

Test Model
```{r}
# Make predictions
pred_classes_svm <- cbb_svm %>% predict(test_data)
# Model accuracy
mean(pred_classes_svm == test_data$WPERC_b)

```


### 3. Decision Tree {#III.3}

Build Model
```{r}
# build model
cbb_tree <- ctree(WPERC_b ~ ., data = train_data)

# call model
summary(cbb_tree)
```

Test Model
```{r}
# Make predictions
pred_classes_tree <- cbb_tree %>% predict(test_data)
# Model accuracy
mean(pred_classes_tree == test_data$WPERC_b)
```


### 4. KNN {#III.4}

Build Model
```{r}
# normalize
norm <-function(x) { (x -min(x))/(max(x)-min(x))}
cbb_norm <- as.data.frame(lapply(subset(cbb_mod, select=-WPERC_b), norm))

# split data 70/30
train_data_norm <- cbb_norm[ind==1,]
test_data_norm <- cbb_norm[ind==2,]

# extract classes
class_train <- train_data$WPERC_b
class_test <- test_data$WPERC_b

# build model
cbb_knn <- knn(train_data_norm, test_data_norm, class_train, k = nbins)

# call model
summary(cbb_knn)
```

Test Model
```{r}
# create the confusion matrix
tb <- table(cbb_knn,class_test)

# check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x))))}
accuracy(tb)
```


### 5. XGBoost {#III.5}

Build Model
```{r}
# Create numeric labels with one-hot encoding
train_labs <- as.numeric(train_data$WPERC_b) - 1
test_labs <- as.numeric(test_data$WPERC_b) - 1

new_train <- model.matrix(~ . + 0, data = train_data[, -1])
new_test <- model.matrix(~ . + 0, data = test_data[, -1])

# Prepare matrices
xgb_train <- xgb.DMatrix(data = new_train, label = train_labs)
xgb_test <- xgb.DMatrix(data = new_test, label = test_labs)

# Set parameters(default)
params <- list(booster = "gbtree", objective = "multi:softprob", num_class = nbins, eval_metric = "mlogloss")

# Calculate # of folds for cross-validation
xgbcv <- xgb.cv(params = params, data = xgb_train, nrounds = 100, nfold = 5, showsd = TRUE, stratified = TRUE, print.every.n = 10, early_stop_round = 20, maximize = FALSE, prediction = TRUE)

# Function to compute classification error
classification_error <- function(conf_mat) {
  conf_mat = as.matrix(conf_mat)
  
  error = 1 - sum(diag(conf_mat)) / sum(conf_mat)
  
  return (error)
}

# Mutate xgb output to deliver hard predictions
xgb_train_preds <- data.frame(xgbcv$pred) %>% mutate(max = max.col(., ties.method = "last"), label = train_labs + 1)

# Examine output
head(xgb_train_preds)
```

Test Model
```{r}
# Confusion Matrix
xgb_conf_mat <- table(true = train_labs + 1, pred = xgb_train_preds$max)

# Model Accuracy
accuracy(xgb_conf_mat)
```


## IV. Visualize and Analyze results {#IV}

```{r}
# plotting.comparing model results

# analyze model accuracy

# model selection

```


Get accuracy when you get all missed
Get accuracy when you remove the missed class. may be much worse

