---
title: '**College Basketball Prediction**'
output:
  html_notebook: default
  pdf_document: default
---
### Data Mining Final Project

**By: Mikul Muzumdar, Alec Plante, Johnson Feng, Leon Masin, Sumukh Shankar**

Data from https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset

`GitHub repo: https://github.com/mikul109/College-Basketball-Prediction`

```{r}
# Dependencies:
library(tidyverse)
library(data.table)
library(classInt)
library(corrplot)
library(leaps)
library(pROC)
library(nnet)
library(e1071)
library(partykit)
library(class)
library(xgboost)
library(caret)
```


<br>
 
| **Objective:**
|   Use historical college basketball data to predict Regular Season Wins

| **Steps:**
|     I. [Explore Dataset](#I)
|     II. [Data Preprocessing](#II)
|         1. [Cleaning](#II.1)
|         2. [Discretization](#II.2)
|         3. [Reducing Dimensionality](#II.3)
|     III. [Build and Test Classification Models](#III)
|         1. [Logistic Regression](#III.1)
|         2. [Support Vector Machines](#III.2)
|         3. [Decision Tree](#III.3)
|         4. [K-Nearest Neighbors](#III.4)
|         5. [XGBoost](#III.5)
|     IV. [Model Selection](#IV)
|         1. [Comparing Accuracy](#IV.1)
|         2. [Comparing AUC](#IV.2)
|         3. [Conclusion](#IV.3)

<br>


## **I. Data Exploration** {#I}

```{r}
# load data
cbb <- read.csv("https://raw.githubusercontent.com/mikul109/College-Basketball-Prediction/main/cbb.csv")
setDT(cbb)
```
Data from the 2013, 2014, 2015, 2016, 2017, 2018, and 2019 D1 College Basketball seasons

```{r}
dim(cbb) 
```
This data has 2455 rows and 24 columns.

<br>

| Our Class Variable:
| `WPERC_b: team's win percentage (number of games won divided by number games played), in 4 bins`

| Other Variables:
| `TEAM: The Division I college basketball school`  
| `CONF: The Athletic Conference in which the school participates in`
| `G: Number of games played`
| `W: Number of games won`
| `ADJOE: Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions)`
|               `a team would have against the average Division I defense)`
| `ADJDE: Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions)`
|               `a team would have against the average Division I offense)`
| `BARTHAG: Power Rating (Chance of beating an average Division I team)`
| `EFG_O: Effective Field Goal Percentage Shot`
| `EFG_D: Effective Field Goal Percentage Allowed`
| `TOR: Turnover Percentage Allowed (Turnover Rate)`
| `TORD: Turnover Percentage Committed (Steal Rate)`
| `ORB: Offensive Rebound Rate`
| `DRB: Offensive Rebound Rate Allowed`
| `FTR : Free Throw Rate (How often the given team shoots Free Throws)`
| `FTRD: Free Throw Rate Allowed`
| `2P_O: Two-Point Shooting Percentage`
| `2P_D: Two-Point Shooting Percentage Allowed`
| `3P_O: Three-Point Shooting Percentage`
| `3P_D: Three-Point Shooting Percentage Allowed`
| `ADJ_T: Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team` 
|               `that wants to play at an average Division I tempo)`
| `WAB: Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)`
| `YEAR: Season`


<br>

```{r}
str(cbb)
```
This shows the type of data and a preview of the data in each column for the dataset. Most of the variables are numeric, but there are also integers and character data types.


```{r}
head(cbb)
```
The head function displays the first 6 rows of the data set.

```{r}
summary(cbb)
```
| The summary function of used on the data set shows the minimum and maximum number of each column, as well as the 1st quartile, 3rd quartile, the mean and the median. 
| This function reveals some important information about our dataset.
| - Not every teams plays the same number of games like in many sports.
| - Not every team in this data set is ranked (no Seed value). This means that they did not make it to the tournament

```{r}
summary(cbb$YEAR)
```
Because COVID messed everything up after 2019, we will use data from 2013 to 2019

```{r}
summary(cbb$W) 
```
The average number of wins is 16.28, Max = 38

```{r}
boxplot(W~YEAR, data = cbb) 
```
Distribution of wins based on the season. These seem to be pretty consistent from year to year



```{r}
boxplot(W~SEED, data = cbb) 
```
This Boxplot shows the distribution of wins based on the seed value. Interestingly, the median number of wins does not always decrease with the seed number. For instance, the median wins for a 12th seed is higher than that for the 5th seed.

This could be due to more games. I am going to create a variable called win percentage and analyze that to see if there are any differences.

```{r}
cbb1<- cbb
cbb1$WPERC<-cbb1$W/cbb1$G

boxplot(WPERC~SEED, data = cbb1) 
```
The same pattern continues. It seems to be even a little more exaggerated with the 12th seed. Lets view the numbers used in this graph
```{r}
by(cbb1$WPERC,cbb1$SEED,summary)
```


```{r}
boxplot(G~YEAR, data = cbb) 
```
Same but for games. As you can see, there is a median of about 31 games per year

Win Percentage Per Conference:
```{r}
ggplot(cbb1, aes(x=CONF, y=WPERC)) + 
  geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


## Scatter Plots {.tabset}

### 1
```{r}
ggplot(cbb) +
  geom_point(aes(EFG_D, W), colour = 'blue')
```

### 2
```{r}
ggplot(cbb) +
  geom_point(aes(ADJDE, W), colour = 'red')
```

### 3
```{r}
ggplot(cbb) +
  geom_point(aes(ADJOE, W), colour = 'purple')
```

### 4
```{r}
ggplot(cbb) +
  geom_point(aes(EFG_D, W), colour = 'cyan')
```

### 5
```{r}
ggplot(cbb) +
  geom_point(aes(TORD, EFG_D), colour = 'green')
```

### 6
```{r}
ggplot(cbb) +
  geom_point(aes(YEAR, X3P_O), colour = 'orange')
```
## {-}

## **II. Data preprocessing** {#II}

## 1. Data Cleaning {#II.1}

```{r}
colSums(is.na(cbb))
```
The only N/A's are in POSTEASON and SEED. Since they only apply to a small percentage of teams that made the tournament, we should drop these columns from our model data


```{r}
# drop POSTSEASON and SEED
cbb2 <- subset(cbb1, select=-c(POSTSEASON, SEED))
```

## 2. Discretization of the Class Variable {#II.2}

```{r}
# Our class variable is win percentage in the regular season, WPERC
# We will use equal frequency binning with 4 bins
nbins <- 4
bins <- classIntervals(cbb2$WPERC, nbins, style = 'quantile')
bins
```
```{r}
# assign bins to the WPERC variable
# assign variables as 1-4, representing the 4 bins
cbb2$WPERC_b <- cut(cbb2$WPERC, breaks = bins$brks, labels=as.character(1:nbins))
summary(cbb2$WPERC_b)
```
```{r}
# drops rows that did not fit into bins
cbb2<-cbb2[!(cbb2$WPERC_b=="NA"),]
summary(cbb2$WPERC_b)

# drop the WPERC variable that is not binned
cbb2<-subset(cbb2, select=-c(WPERC))
```



## 3. Reducing Dimensionality/Variable Selection {#II.3}

First, we should determine which variables outside of WPERC_b are independent. 
```{r fig.width=10}
# remove class and string variables
cbb_var <- subset(cbb2, select = -c(TEAM, CONF, WPERC_b))

# remove obvious dependent variables
cbb_var1 <- subset(cbb_var, select = -c(W, G, WAB))

# get correlations between each other 
cbb_cor <- cor(cbb_var1)

# plot
corrplot(cbb_cor, method="circle")
```

Next, we should see which variables are correlated with the class variable. 
```{r fig.width=10}
# run linear regression and plot r^2
m1<-regsubsets(WPERC_b~.-TEAM-YEAR-CONF-G-W-WAB, data = cbb2)
plot(m1,scale="adjr2", main="Variable Selection Based on adjr2")
```

| Conclusions:
| - Offensive stats such as ADJOE, 2P_O, and 3P_O are highly correlated with each other
| - Defensive stats such as ADJDE, EFG_D, 2P_D, and 3P_D are highly correlated with each other
| - BARTHAG is the variable most correlated with the class variable, WPERC_b
| - BARTHAG is most correlated with ADJOE and ADJDE


<br>

| Final Variables, based on independence, correlation with WPERC_b, and general basketball knowledge:
| 1. BARTHAG
| 2. EFG_O
| 3. EFG_D
| 4. TORD
| 5. DRB
| 6. ADJOE
| 7. ADJDE


## **III. Build and Test Classification Models** {#III}

Split into Train and Test datasets
```{r}
# convert class variable to factor
cbb2$WPERC_b <- as.factor(cbb2$WPERC_b)

# subset with needed variables
cbb_mod <- subset(cbb2, select=c(WPERC_b, BARTHAG, EFG_O, EFG_D, TORD, DRB, ADJOE, ADJDE))

# randomly split data 70/30
set.seed(123)
ind <- sample(2, nrow(cbb_mod), replace=TRUE, prob=(c(0.7,0.3)))
train_data <- cbb_mod[ind==1,]
test_data <- cbb_mod[ind==2,]
```



## 1. Logistic Regression {#III.1} 
## {.tabset}

### Build Model
```{r}
# build model
cbb_glm <- multinom(WPERC_b ~ ., data = train_data)

# call model
summary(cbb_glm)
```
### Plot Model
```{r fig.width=10}
# blank for now
```

### Test Model
```{r}
# Make predictions
pred_classes_glm <- cbb_glm %>% predict(test_data)
# Model accuracy
mean(pred_classes_glm == test_data$WPERC_b)

# Confusion matrix and stats
confusionMatrix(pred_classes_glm, test_data$WPERC_b)
```
### ROC Curve
```{r, results='hide', warning=FALSE, fig.width=10}
# define object to plot and calculate AUC
predicted_glm <- factor(pred_classes_glm, ordered = TRUE)
rocobj_glm <- roc(test_data$WPERC_b, predicted_glm)
auc_glm <- round(auc(test_data$WPERC_b, predicted_glm),4)

# create ROC plot
ggroc(rocobj_glm, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc_glm, ')'))
```
## {-}

```{r}

#cbb3<-cbb
# All teams that missed the playoffs are given a value of 0
#cbb3$SEED[cbb3$SEED==100]<-0
# All teams that made the playoffs are given a value of 1
#cbb3$SEED[cbb3$SEED>0]<-1
#View(cbb3)
# subset with needed variables
#cbb3_mod <- subset(cbb3, select=c(SEED, W, ADJOE, ADJDE, EFG_O, EFG_D, DRB))

# randomly split data 70/30
#set.seed(123)
#ind <- sample(2, nrow(cbb3_mod), replace=TRUE, prob=(c(0.7,0.3)))
#train_data_cbb3 <- cbb3_mod[ind==1,]
#test_data_cbb3 <- cbb3_mod[ind==2,]


#cbblmBinary<-glm(SEED~W, data =train_data_cbb3, family="binomial")
#summary(cbblmBinary)
# Make predictions
#pred_classes_lmBin <- cbblmBinary %>% predict(test_data_cbb3)
# Model accuracy
#pred_
```


## 2. SVM {#III.2}
## {.tabset}

### Build Model
```{r}
cbb_svm = svm(WPERC_b ~ ., data = train_data, type="C-classification", kernal="radial", gamma=0.1, cost=10, scale = TRUE)

summary(cbb_svm)
```

### Plot Model
```{r fig.width=10}
plot(cbb_svm, train_data, EFG_D~BARTHAG)
```

### Test Model
```{r}
# Make predictions
pred_classes_svm <- cbb_svm %>% predict(test_data)
# Model accuracy
mean(pred_classes_svm == test_data$WPERC_b)

# Confusion matrix and stats
confusionMatrix(pred_classes_svm, test_data$WPERC_b)
```

### ROC Curve
```{r, results='hide', warning=FALSE, fig.width=10}
# define object to plot and calculate AUC
predicted_svm <- factor(pred_classes_svm, ordered = TRUE)
rocobj_svm <- roc(test_data$WPERC_b, predicted_svm)
auc_svm <- round(auc(test_data$WPERC_b, predicted_svm),4)

# create ROC plot
ggroc(rocobj_svm, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc_svm, ')'))
```
## {-}

## 3. Decision Tree {#III.3}
## {.tabset}

### Build Model
```{r}
# build model
cbb_tree <- ctree(WPERC_b ~ ., data = train_data)

# call model
summary(cbb_tree)
```

### Plot Model
```{r fig.width=10}
plot(cbb_tree)
```


### Test Model
```{r}
# Make predictions
pred_classes_tree <- cbb_tree %>% predict(test_data)
# Model accuracy
mean(pred_classes_tree == test_data$WPERC_b)

# Confusion matrix and stats
confusionMatrix(pred_classes_tree, test_data$WPERC_b)
```
### ROC Curve
```{r, results='hide', warning=FALSE, fig.width=10}
# define object to plot and calculate AUC
predicted_tree <- factor(pred_classes_tree, ordered = TRUE)
rocobj_tree <- roc(test_data$WPERC_b, predicted_tree)
auc_tree <- round(auc(test_data$WPERC_b, predicted_tree),4)

# create ROC plot
ggroc(rocobj_tree, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc_tree, ')'))
```
## {-}


## 4. KNN {#III.4}
## {.tabset}

### Build Model
```{r}
# normalize
norm <-function(x) { (x -min(x))/(max(x)-min(x))}
cbb_norm <- as.data.frame(lapply(subset(cbb_mod, select=-WPERC_b), norm))

# split data 70/30
train_data_norm <- cbb_norm[ind==1,]
test_data_norm <- cbb_norm[ind==2,]

# extract classes
class_train <- train_data$WPERC_b
class_test <- test_data$WPERC_b

# build model
cbb_knn <- knn(train_data_norm, test_data_norm, class_train, k = nbins)

# call model
summary(cbb_knn)
```
### Plot Model
```{r fig.width=10}
# blank for now
```

### Test Model
```{r}
# create the confusion matrix
tb_knn <- table(cbb_knn,class_test)

# check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x))))}
accuracy(tb_knn)

# Confusion matrix and stats
confusionMatrix(cbb_knn, test_data$WPERC_b)
```
### ROC Curve
```{r, results='hide', warning=FALSE, fig.width=10}
# define object to plot and calculate AUC
predicted_knn <- factor(cbb_knn, ordered = TRUE)
rocobj_knn <- roc(test_data$WPERC_b, predicted_knn)
auc_knn <- round(auc(test_data$WPERC_b, predicted_knn),4)

# create ROC plot
ggroc(rocobj_knn, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc_knn, ')'))
```
## {-}


## 5. XGBoost {#III.5}
## {.tabset}

### Build Model
```{r}
# Create numeric labels with one-hot encoding
train_labs <- as.numeric(train_data$WPERC_b) - 1
test_labs <- as.numeric(test_data$WPERC_b) - 1

new_train <- model.matrix(~ . + 0, data = train_data[, -1])
new_test <- model.matrix(~ . + 0, data = test_data[, -1])

# Prepare matrices
xgb_train <- xgb.DMatrix(data = new_train, label = train_labs)
xgb_test <- xgb.DMatrix(data = new_test, label = test_labs)

# Set parameters(default)
params <- list(booster = "gbtree", objective = "multi:softprob", num_class = nbins, eval_metric = "mlogloss")

# Calculate # of folds for cross-validation
xgbcv <- xgb.cv(params = params, data = xgb_train, nrounds = 100, nfold = 5, showsd = TRUE, stratified = TRUE, print.every.n = 10, early_stop_round = 20, maximize = FALSE, prediction = TRUE)

# Function to compute classification error
classification_error <- function(conf_mat) {
  conf_mat = as.matrix(conf_mat)
  error = 1 - sum(diag(conf_mat)) / sum(conf_mat)
  return (error)
}

# Mutate xgb output to deliver hard predictions
xgb_train_preds <- data.frame(xgbcv$pred) %>% mutate(max = max.col(., ties.method = "last"), label = train_labs + 1)

# Examine output
head(xgb_train_preds)
```

### Plot Model
```{r fig.width=10}
# blank for now
```


### Test Model
```{r}
# Confusion Matrix
xgb_conf_mat <- table(true = train_labs + 1, pred = xgb_train_preds$max)

# Model Accuracy
accuracy(xgb_conf_mat)

# Confusion matrix and stats
confusionMatrix(factor(xgb_train_preds$max), factor(train_labs + 1))
```
### ROC Curve
```{r, results='hide', warning=FALSE, fig.width=10}
# define object to plot and calculate AUC
predicted_xgb <- factor(xgb_train_preds$max, ordered = TRUE)
rocobj_xgb <- roc(factor(train_labs + 1), predicted_xgb)
auc_xgb <- round(auc(factor(train_labs + 1), predicted_xgb),4)

# create ROC plot
ggroc(rocobj_xgb, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc_xgb, ')'))
```
## {-}

## **IV. Model Selection** {#IV}

### Comparing Accuracy {#IV.1}
```{r}
cat('MEAN ACCURACY:',
paste0('Logistic Regression: ', mean(pred_classes_glm == test_data$WPERC_b)),
paste0('Support Vector Machines: ', mean(pred_classes_svm == test_data$WPERC_b)),
paste0('Decision Tree: ', mean(pred_classes_tree == test_data$WPERC_b)),
paste0('K-Nearest Neighbors: ', accuracy(tb_knn)),
paste0('XGBoost: ', accuracy(xgb_conf_mat)), 
sep = '\n')
```

### Comparing AUC {#IV.2}
```{r}
cat('AUC:',
paste0('Logistic Regression: ', auc_glm),
paste0('Support Vector Machines: ', auc_svm),
paste0('Decision Tree: ', auc_tree),
paste0('K-Nearest Neighbors: ', auc_knn),
paste0('XGBoost: ', auc_xgb), 
sep = '\n')
```

### Conclusions {#IV.3}
| - All models perform okay, with ~0.60 mean accuracy, and ~0.80 AUC
| - This is expected given how unpredictable college basketball is, and since advanced stats do not directly
|   predict wins, as shown in the variable selection section.
| - SVM has the highest mean accuracy, with Logistic Regression performing slightly worse
| - Logistic Regression has the best AUC, with SVM slightly lower
| - The best model is between SVM and Logistic Regression. 

<br>

