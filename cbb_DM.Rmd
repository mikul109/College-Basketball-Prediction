---
title: '**College Basketball Prediction**'
output:
  html_notebook: default
  pdf_document: default
---
### Data Mining Final Project

**By: Mikul Muzumdar, Alec Plante, Johnson Feng, Leon Masin, Sumukh Shankar**

Data from https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset

`GitHub repo: https://github.com/mikul109/College-Basketball-Prediction`

```{r}
# Dependencies:
library(tidyverse)
library(data.table)
library(plotly)
library(corrplot)
library(leaps)
library(nnet)
library(e1071)
library(partykit)
library(class)
library(xgboost)
library(caret)
```


<br>
 
| **Objective:**
|   Use historical college basketball data to predict March Madness results

| **Steps:**
|     I. [Explore Dataset](#I)
|     II. [Data Preprocessing](#II)
|         1. [Cleaning](#II.1)
|         2. [Reduce Dimensionality](#II.2)
|     III. [Build and Test Classification Models](#III)
|         1. [Logistic Regression](#III.1)
|         2. [Support Vector Machine](#III.2)
|         3. [Decision Tree](#III.3)
|         4. [K-Nearest-Neighbors](#III.4)
|         5. [XGBoost](#III.5)
|     IV. [Visualize and Analyze results](#IV)

<br>


## I. Data Exploration {#I}

```{r}
# load data
cbb <- read.csv("https://raw.githubusercontent.com/mikul109/College-Basketball-Prediction/main/cbb.csv")
setDT(cbb)
```
Data from the 2013, 2014, 2015, 2016, 2017, 2018, and 2019 D1 College Basketball seasons

```{r}
dim(cbb) 
```
This data has 2455 rows and 24 columns


```{r}
str(cbb)
```
This shows the type of data and a preview of the data in each column for the dataset. Most of the variables are numeric, but there are also integers and character data types.


```{r}
head(cbb)
```
The head function displays the first 6 rows of the data set.

```{r}
summary(cbb)
```
The summary function of used on the data set shows the minimum and maximum number of each column, as well as the 1st quartile, 3rd quartile, the mean and the median. 
This function reveals some important information about our dataset.
- Not every teams plays the same number of games like in many sports.
- NOt every team in this data set is ranked (no Seed value). This means that they did not make it to the tournament




```{r}
summary(cbb$YEAR)
```
Because COVID messed everything up after 2019, we will use data from 2013 to 2019

```{r}
summary(cbb$W) 
```
The average number of wins is 16.28, Max = 38

```{r}
boxplot(W~YEAR, data = cbb) 
```
Distribution of wins based on the season. These seem to be pretty consistent from year to year



```{r}
boxplot(W~SEED, data = cbb) 
```
This Boxplot shows the distribution of wins based on the seed value. Interestingly, the median number of wins does not always decrease with the seed number. For instance, the median wins for a 12th seed is higher than that for the 5th seed.

This could be due to more games. I am going to create a variable called win percentage and analyze that to see if there are any differences.

```{r}
cbb1<- cbb
cbb1$WPERC<-cbb1$W/cbb1$G

boxplot(WPERC~SEED, data = cbb1) 
```
The same pattern continues. It seems to be even a little more exagerated with the 12th seed. Lets view the numbers used in this graph
```{r}
by(cbb1$WPERC,cbb1$SEED,summary)
```


```{r}
boxplot(G~YEAR, data = cbb) 
```
Same but for games. As you can see, there is a median of about 31 games per year


## II. Data preprocessing {#II}

### 1. Data Cleaning {#II.1}

```{r}
colSums(is.na(cbb))
```
The only N/A's are in POSTEASON and SEED. These are meaningful, however, since they indicate which teams made the tournament or not.


```{r}
# change n/a to "MISSED" the tourney
cbb$POSTSEASON[is.na(cbb$POSTSEASON)]<-"MISSED"
# change n/a in seed to a large number, like 100
cbb$SEED[is.na(cbb$SEED)]<-100
```



### 2. Reducing Dimensionality/Variable Selection {#II.2}

| Our Class Variable:
| `POSTSEASON: Round where the given team was eliminated or where their season ended`

| Other Variables:
| `TEAM: The Division I college basketball school`  
| `CONF: The Athletic Conference in which the school participates in`
| `G: Number of games played`
| `W: Number of games won`
| `ADJOE: Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions)`
|               `a team would have against the average Division I defense)`
| `ADJDE: Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions)`
|               `a team would have against the average Division I offense)`
| `BARTHAG: Power Rating (Chance of beating an average Division I team)`
| `EFG_O: Effective Field Goal Percentage Shot`
| `EFG_D: Effective Field Goal Percentage Allowed`
| `TOR: Turnover Percentage Allowed (Turnover Rate)`
| `TORD: Turnover Percentage Committed (Steal Rate)`
| `ORB: Offensive Rebound Rate`
| `DRB: Offensive Rebound Rate Allowed`
| `FTR : Free Throw Rate (How often the given team shoots Free Throws)`
| `FTRD: Free Throw Rate Allowed`
| `2P_O: Two-Point Shooting Percentage`
| `2P_D: Two-Point Shooting Percentage Allowed`
| `3P_O: Three-Point Shooting Percentage`
| `3P_D: Three-Point Shooting Percentage Allowed`
| `ADJ_T: Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team` 
|               `that wants to play at an average Division I tempo)`
| `WAB: Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)`
| `SEED: Seed in the NCAA March Madness Tournament`
| `YEAR: Season`


<br>
First, we should determine which variables outside of POSTSEASON are independent. 
```{r fig.width=10}
# remove class and string variables
cbb_var = subset(cbb, select = -c(TEAM, CONF, POSTSEASON))

# get correlations between each other 
cbb_cor = cor(cbb_var)

# plot
corrplot(cbb_cor, method="circle")
```

Next, we should see which variables are correlated with the class variable. 
```{r fig.width=10}
# set texts to numbers
cbb1 <- cbb
cbb1$POSTSEASON[cbb1$POSTSEASON=="MISSED"]<--1
cbb1$POSTSEASON[cbb1$POSTSEASON=="R68"]<-0
cbb1$POSTSEASON[cbb1$POSTSEASON=="R64"]<-1
cbb1$POSTSEASON[cbb1$POSTSEASON=="R32"]<-2
cbb1$POSTSEASON[cbb1$POSTSEASON=="S16"]<-3
cbb1$POSTSEASON[cbb1$POSTSEASON=="E8"]<-4
cbb1$POSTSEASON[cbb1$POSTSEASON=="F4"]<-5
cbb1$POSTSEASON[cbb1$POSTSEASON=="2ND"]<-6
cbb1$POSTSEASON[cbb1$POSTSEASON=="Champions"]<-7

# run linear regression and plot r^2
m1<-regsubsets(POSTSEASON~.-TEAM-YEAR-CONF-G, data = cbb1)
plot(m1,scale="adjr2", main="Variable Selection Based on adjr2")
```

| Conclusions:
| - Wins, BARTHAG, and Wins above Bubble are highly correlated
| - Offensive Effective FG is highly correlated with 3pt% and 2pt%
| - Seed and Wins are negatively correlated
| - No variable is highly correlated with the class variable, as expected of March Madness, which is known to be unpredictable

<br>

| Final Variables, based on independence, correlation with POSTSEASON, and general basketball knowledge:
| 1. W
| 2. ADJOE
| 3. ADJDE
| 4. EFG_O
| 5. EFG_D
| 6. DRB


## III. Build and Test Classification Models {#III}

Split into Train and Test datasets
```{r}
# convert class variable to factor
cbb$POSTSEASON <- as.factor(cbb$POSTSEASON)

# subset with needed variables
cbb_mod <- subset(cbb, select=c(POSTSEASON, W, ADJOE, ADJDE, EFG_O, EFG_D, DRB))

# randomly split data 70/30
set.seed(123)
ind <- sample(2, nrow(cbb_mod), replace=TRUE, prob=(c(0.7,0.3)))
train_data <- cbb_mod[ind==1,]
test_data <- cbb_mod[ind==2,]
```


Before running models, I am going to create a baseline accuracy. Since this dataset is dominated by teams not making the tournament, the results from the models should be more than this percent:
```{r}
View(cbb)
nrow(cbb[cbb$SEED==100])/nrow(cbb)
```

### 1. Logistic Regression {#III.1}

Build Model
```{r}
# build model
cbb_glm <- multinom(POSTSEASON ~ ., data = train_data)

# call model
summary(cbb_glm)

```

Test Model
```{r}
# Make predictions
pred_classes_glm <- cbb_glm %>% predict(test_data)
# Model accuracy
mean(pred_classes_glm == test_data$POSTSEASON)
```


### 2. SVM {#III.2}

Build Model
```{r}
cbb_svm = svm(POSTSEASON ~ ., data = train_data, type="C-classification", kernal="radial", gamma=0.1, cost=10, scale = TRUE)

summary(cbb_svm)
```

Test Model
```{r}
# Make predictions
pred_classes_svm <- cbb_svm %>% predict(test_data)
# Model accuracy
mean(pred_classes_svm == test_data$POSTSEASON)

```


### 3. Decision Tree {#III.3}

Build Model
```{r}
# build model
cbb_tree <- ctree(POSTSEASON ~ ., data = train_data)

# call model
summary(cbb_tree)
```

Test Model
```{r}
# Make predictions
pred_classes_tree <- cbb_tree %>% predict(test_data)
# Model accuracy
mean(pred_classes_tree == test_data$POSTSEASON)
```


### 4. KNN {#III.4}

Build Model
```{r}
# normalize
norm <-function(x) { (x -min(x))/(max(x)-min(x))}
cbb_norm <- as.data.frame(lapply(subset(cbb_mod, select=-POSTSEASON), norm))

# split data 70/30
train_data_norm <- cbb_norm[ind==1,]
test_data_norm <- cbb_norm[ind==2,]

# extract classes
class_train <- train_data$POSTSEASON
class_test <- test_data$POSTSEASON

# build model
cbb_knn <- knn(train_data_norm, test_data_norm, class_train, k = 9)

# call model
summary(cbb_knn)
```

Test Model
```{r}
# create the confusion matrix
tb <- table(cbb_knn,class_test)

# check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x))))}
accuracy(tb)
```


### 5. XGBoost {#III.5}

Build Model
```{r}
# Create numeric labels with one-hot encoding
train_labs <- as.numeric(train_data$POSTSEASON) - 1
test_labs <- as.numeric(test_data$POSTSEASON) - 1

new_train <- model.matrix(~ . + 0, data = train_data[, -1])
new_test <- model.matrix(~ . + 0, data = test_data[, -1])

# Prepare matrices
xgb_train <- xgb.DMatrix(data = new_train, label = train_labs)
xgb_test <- xgb.DMatrix(data = new_test, label = test_labs)

# Set parameters(default)
params <- list(booster = "gbtree", objective = "multi:softprob", num_class = 9, eval_metric = "mlogloss")

# Calculate # of folds for cross-validation
xgbcv <- xgb.cv(params = params, data = xgb_train, nrounds = 100, nfold = 5, showsd = TRUE, stratified = TRUE, print.every.n = 10, early_stop_round = 20, maximize = FALSE, prediction = TRUE)

# Function to compute classification error
classification_error <- function(conf_mat) {
  conf_mat = as.matrix(conf_mat)
  
  error = 1 - sum(diag(conf_mat)) / sum(conf_mat)
  
  return (error)
}

# Mutate xgb output to deliver hard predictions
xgb_train_preds <- data.frame(xgbcv$pred) %>% mutate(max = max.col(., ties.method = "last"), label = train_labs + 1)

# Examine output
head(xgb_train_preds)
```

Test Model
```{r}
# Confusion Matrix
xgb_conf_mat <- table(true = train_labs + 1, pred = xgb_train_preds$max)

# Model Accuracy
accuracy(xgb_conf_mat)
```


## IV. Visualize and Analyze results {#IV}

```{r}
# plotting.comparing model results

# analyze model accuracy

# model selection

```


Get accuracy when you get all missed
Get accuracy when you remove the missed class. may be much worse

